{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It89APiAtTUF"
      },
      "source": [
        "# Meeting Minutes Generator with Gradio Interface\n",
        "\n",
        "This notebook creates a user-friendly Gradio interface for uploading audio files and generating meeting minutes automatically. It uses:\n",
        "- OpenAI's Whisper model for audio transcription\n",
        "- Local LLM (Phi-3) for generating structured meeting minutes\n",
        "- Gradio for the web interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2vvgnFpHpID"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW8nl3XRFrz0"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import io\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3D1_T0uG_Qh"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "AUDIO_MODEL = \"whisper-1\"\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYW8kQYtF-3L"
      },
      "outputs": [],
      "source": [
        "# Setup authentication - replace with your own API keys\n",
        "# For Colab, use userdata.get('KEY_NAME')\n",
        "# For local Jupyter, use os.getenv('KEY_NAME') or input directly\n",
        "\n",
        "try:\n",
        "    # Try Colab first\n",
        "    from google.colab import userdata\n",
        "    from huggingface_hub import login\n",
        "    \n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    \n",
        "    login(hf_token, add_to_git_credential=True)\n",
        "    print(\"Using Colab authentication\")\n",
        "except ImportError:\n",
        "    # Local environment\n",
        "    hf_token = os.getenv('HF_TOKEN')\n",
        "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "    \n",
        "    if hf_token:\n",
        "        from huggingface_hub import login\n",
        "        login(hf_token, add_to_git_credential=True)\n",
        "    \n",
        "    print(\"Using local environment authentication\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "if openai_api_key:\n",
        "    openai_client = OpenAI(api_key=openai_api_key)\n",
        "    print(\"OpenAI client initialized successfully\")\n",
        "else:\n",
        "    print(\"Warning: OpenAI API key not found. Audio transcription will not work.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcRKUgcxMew6"
      },
      "outputs": [],
      "source": [
        "# Setup quantization configuration for efficient model loading\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CujZRAgMimy"
      },
      "outputs": [],
      "source": [
        "# Load the language model and tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(PHI3)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    PHI3, \n",
        "    device_map=\"auto\", \n",
        "    quantization_config=quant_config\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transcribe_audio"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio(audio_file):\n",
        "    \"\"\"\n",
        "    Transcribe audio file using OpenAI's Whisper model\n",
        "    \n",
        "    Args:\n",
        "        audio_file: File path or file-like object\n",
        "    \n",
        "    Returns:\n",
        "        str: Transcribed text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not openai_client:\n",
        "            return \"Error: OpenAI API key not configured\"\n",
        "        \n",
        "        # Handle different input types\n",
        "        if isinstance(audio_file, str):\n",
        "            # File path\n",
        "            with open(audio_file, \"rb\") as f:\n",
        "                transcription = openai_client.audio.transcriptions.create(\n",
        "                    model=AUDIO_MODEL,\n",
        "                    file=f,\n",
        "                    response_format=\"text\"\n",
        "                )\n",
        "        else:\n",
        "            # File-like object (from Gradio)\n",
        "            with open(audio_file.name, \"rb\") as f:\n",
        "                transcription = openai_client.audio.transcriptions.create(\n",
        "                    model=AUDIO_MODEL,\n",
        "                    file=f,\n",
        "                    response_format=\"text\"\n",
        "                )\n",
        "        \n",
        "        return transcription\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error transcribing audio: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_minutes"
      },
      "outputs": [],
      "source": [
        "def generate_meeting_minutes(transcription):\n",
        "    \"\"\"\n",
        "    Generate structured meeting minutes from transcription using local LLM\n",
        "    \n",
        "    Args:\n",
        "        transcription (str): Audio transcription text\n",
        "    \n",
        "    Returns:\n",
        "        str: Generated meeting minutes in markdown format\n",
        "    \"\"\"\n",
        "    try:\n",
        "        system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
        "        \n",
        "        user_prompt = f\"\"\"Below is a transcript of a meeting. Please write professional meeting minutes in markdown format, including:\n",
        "        \n",
        "        1. **Meeting Summary** - Date, attendees (if mentioned), location/platform\n",
        "        2. **Key Discussion Points** - Main topics covered\n",
        "        3. **Decisions Made** - Important decisions reached\n",
        "        4. **Action Items** - Tasks assigned with owners (if mentioned)\n",
        "        5. **Next Steps** - Follow-up actions or next meeting\n",
        "        \n",
        "        Transcript:\n",
        "        {transcription}\"\"\"\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "        \n",
        "        # Prepare input for the model\n",
        "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "        \n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs, \n",
        "                max_new_tokens=2000,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode the response\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract only the assistant's response (remove the input prompt)\n",
        "        # Find the last assistant response\n",
        "        if \"<|assistant|>\" in full_response:\n",
        "            response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
        "        else:\n",
        "            # Fallback: try to extract after the user prompt\n",
        "            response = full_response[full_response.find(user_prompt) + len(user_prompt):].strip()\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error generating meeting minutes: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "process_audio"
      },
      "outputs": [],
      "source": [
        "def process_audio_file(audio_file, progress=gr.Progress()):\n",
        "    \"\"\"\n",
        "    Main function to process audio file and generate meeting minutes\n",
        "    \n",
        "    Args:\n",
        "        audio_file: Uploaded audio file from Gradio\n",
        "        progress: Gradio progress indicator\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (transcription, meeting_minutes)\n",
        "    \"\"\"\n",
        "    if audio_file is None:\n",
        "        return \"No audio file uploaded.\", \"Please upload an audio file first.\"\n",
        "    \n",
        "    try:\n",
        "        progress(0.1, desc=\"Processing audio file...\")\n",
        "        \n",
        "        # Step 1: Transcribe audio\n",
        "        progress(0.3, desc=\"Transcribing audio...\")\n",
        "        transcription = transcribe_audio(audio_file)\n",
        "        \n",
        "        if transcription.startswith(\"Error\"):\n",
        "            return transcription, \"Transcription failed - cannot generate minutes.\"\n",
        "        \n",
        "        progress(0.6, desc=\"Generating meeting minutes...\")\n",
        "        \n",
        "        # Step 2: Generate meeting minutes\n",
        "        meeting_minutes = generate_meeting_minutes(transcription)\n",
        "        \n",
        "        progress(1.0, desc=\"Complete!\")\n",
        "        \n",
        "        return transcription, meeting_minutes\n",
        "    \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error processing audio: {str(e)}\"\n",
        "        return error_msg, error_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gradio_interface"
      },
      "outputs": [],
      "source": [
        "# Create Gradio interface\n",
        "def create_interface():\n",
        "    \"\"\"\n",
        "    Create and configure the Gradio interface\n",
        "    \"\"\"\n",
        "    \n",
        "    # Custom CSS for better styling\n",
        "    custom_css = \"\"\"\n",
        "    .gradio-container {\n",
        "        max-width: 1200px !important;\n",
        "        margin: auto !important;\n",
        "    }\n",
        "    .tab-nav button {\n",
        "        font-size: 16px !important;\n",
        "        padding: 12px 24px !important;\n",
        "    }\n",
        "    .markdown-text {\n",
        "        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important;\n",
        "    }\n",
        "    \"\"\"\n",
        "    \n",
        "    with gr.Blocks(css=custom_css, title=\"Meeting Minutes Generator\") as interface:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # 🎤 Meeting Minutes Generator\n",
        "            \n",
        "            Upload an audio file from your meeting, and this tool will:\n",
        "            1. **Transcribe** the audio using OpenAI's Whisper model\n",
        "            2. **Generate** structured meeting minutes using AI\n",
        "            \n",
        "            **Supported formats**: MP3, WAV, M4A, MP4, WEBM, and more\n",
        "            \"\"\"\n",
        "        )\n",
        "        \n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Audio upload section\n",
        "                gr.Markdown(\"### 📁 Upload Audio File\")\n",
        "                audio_input = gr.File(\n",
        "                    label=\"Select your meeting audio file\",\n",
        "                    file_types=[\"audio\"],\n",
        "                    file_count=\"single\"\n",
        "                )\n",
        "                \n",
        "                process_btn = gr.Button(\n",
        "                    \"🚀 Process Audio & Generate Minutes\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "                \n",
        "                # Status indicator\n",
        "                status = gr.Textbox(\n",
        "                    label=\"Status\",\n",
        "                    value=\"Ready to process audio file\",\n",
        "                    interactive=False\n",
        "                )\n",
        "        \n",
        "        # Results section\n",
        "        gr.Markdown(\"### 📄 Results\")\n",
        "        \n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"📝 Meeting Minutes\"):\n",
        "                minutes_output = gr.Markdown(\n",
        "                    label=\"Generated Meeting Minutes\",\n",
        "                    value=\"Meeting minutes will appear here after processing...\",\n",
        "                    elem_classes=[\"markdown-text\"]\n",
        "                )\n",
        "                \n",
        "                # Download button for minutes\n",
        "                download_minutes = gr.File(\n",
        "                    label=\"Download Meeting Minutes\",\n",
        "                    visible=False\n",
        "                )\n",
        "            \n",
        "            with gr.Tab(\"📄 Full Transcription\"):\n",
        "                transcription_output = gr.Textbox(\n",
        "                    label=\"Audio Transcription\",\n",
        "                    value=\"Transcription will appear here after processing...\",\n",
        "                    lines=15,\n",
        "                    max_lines=20,\n",
        "                    interactive=False\n",
        "                )\n",
        "        \n",
        "        # Event handlers\n",
        "        def update_status(audio_file):\n",
        "            if audio_file is None:\n",
        "                return \"Please select an audio file\"\n",
        "            return f\"Audio file loaded: {audio_file.name if hasattr(audio_file, 'name') else 'file.audio'} - Ready to process\"\n",
        "        \n",
        "        def process_and_save(audio_file, progress=gr.Progress()):\n",
        "            \"\"\"Process audio and return results with download file\"\"\"\n",
        "            transcription, minutes = process_audio_file(audio_file, progress)\n",
        "            \n",
        "            # Create download file if minutes were generated successfully\n",
        "            download_file = None\n",
        "            if minutes and not minutes.startswith(\"Meeting minutes will appear\") and not minutes.startswith(\"Error\"):\n",
        "                temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False)\n",
        "                temp_file.write(minutes)\n",
        "                temp_file.close()\n",
        "                download_file = temp_file.name\n",
        "            \n",
        "            return transcription, minutes, download_file\n",
        "        \n",
        "        # Wire up the interface\n",
        "        audio_input.change(\n",
        "            fn=update_status,\n",
        "            inputs=[audio_input],\n",
        "            outputs=[status]\n",
        "        )\n",
        "        \n",
        "        process_btn.click(\n",
        "            fn=process_and_save,\n",
        "            inputs=[audio_input],\n",
        "            outputs=[transcription_output, minutes_output, download_minutes],\n",
        "            show_progress=True\n",
        "        )\n",
        "        \n",
        "        # Add examples section\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            ### 💡 Tips for Best Results\n",
        "            \n",
        "            - **Audio Quality**: Use clear, high-quality audio recordings\n",
        "            - **File Size**: Large files may take longer to process\n",
        "            - **Content**: Works best with structured meetings and clear speech\n",
        "            - **Languages**: Optimized for English, but supports multiple languages\n",
        "            \n",
        "            ### 🔧 Technical Details\n",
        "            - **Transcription**: OpenAI Whisper model\n",
        "            - **Minutes Generation**: Microsoft Phi-3 local model\n",
        "            - **Privacy**: Audio processing happens on this server\n",
        "            \"\"\"\n",
        "        )\n",
        "    \n",
        "    return interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_interface"
      },
      "outputs": [],
      "source": [
        "# Create and launch the Gradio interface\n",
        "print(\"Creating Gradio interface...\")\n",
        "demo = create_interface()\n",
        "\n",
        "print(\"Launching interface...\")\n",
        "# Launch the interface\n",
        "demo.launch(\n",
        "    share=True,  # Creates a public link for sharing\n",
        "    debug=True,  # Enable debug mode for development\n",
        "    server_name=\"0.0.0.0\",  # Allow external access\n",
        "    server_port=7860,  # Default Gradio port\n",
        "    show_error=True  # Show detailed error messages\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_instructions"
      },
      "source": [
        "## 📋 Usage Instructions\n",
        "\n",
        "1. **Upload Audio**: Click on the file upload area and select your meeting audio file\n",
        "2. **Process**: Click the \"Process Audio & Generate Minutes\" button\n",
        "3. **View Results**: Switch between the \"Meeting Minutes\" and \"Full Transcription\" tabs\n",
        "4. **Download**: Use the download button to save your meeting minutes as a markdown file\n",
        "\n",
        "## 🚀 Features\n",
        "\n",
        "- **Easy Upload**: Drag and drop or browse for audio files\n",
        "- **Progress Tracking**: Real-time progress updates during processing\n",
        "- **Professional Output**: Structured meeting minutes with key sections\n",
        "- **Download Option**: Save results as markdown files\n",
        "- **Multiple Formats**: Supports various audio formats (MP3, WAV, M4A, etc.)\n",
        "\n",
        "## 🔧 Customization\n",
        "\n",
        "You can customize this notebook by:\n",
        "\n",
        "- **Changing Models**: Replace PHI3 with other language models\n",
        "- **Modifying Prompts**: Edit the system message and user prompts for different output styles\n",
        "- **Adding Features**: Include speaker identification, sentiment analysis, etc.\n",
        "- **Styling**: Modify the CSS for different visual themes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## 🛠️ Troubleshooting\n",
        "\n",
        "### Common Issues:\n",
        "\n",
        "1. **\"OpenAI API key not configured\"**\n",
        "   - Make sure to set your OpenAI API key in the environment variables\n",
        "   - For Colab: Add it to Secrets as 'OPENAI_API_KEY'\n",
        "   - For local: Set environment variable or modify the authentication cell\n",
        "\n",
        "2. **\"CUDA out of memory\"**\n",
        "   - Try using a smaller model or reduce batch size\n",
        "   - Restart the runtime and run cells again\n",
        "\n",
        "3. **\"File format not supported\"**\n",
        "   - Convert your audio to MP3, WAV, or M4A format\n",
        "   - Use online converters or audio editing software\n",
        "\n",
        "4. **\"Model loading failed\"**\n",
        "   - Check your HuggingFace token permissions\n",
        "   - Ensure you have enough disk space\n",
        "   - Try restarting the runtime\n",
        "\n",
        "### Performance Tips:\n",
        "- Use shorter audio files (< 1 hour) for faster processing\n",
        "- Ensure good audio quality for better transcription\n",
        "- Close other applications to free up GPU memory"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
